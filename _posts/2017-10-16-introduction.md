---
layout: post
title: "简介"
date: 2017-10-16
description: "强化学习笔记系列的简介"
tag: 强化学习 
---  

这几年人工智能火爆，近期也因为项目的原因开始接触机器学习。目前学的是强化学习，为了对学过的知识进行整理总结，准备以博客的形式写出来，主要内容是David Silver的公开课。限于水平如有错误欢迎指正！
<p>**强化学习**（Reinforcement Learning）也叫增强学习，是一种机器学习方法。有一种机器学习分类是把学习算法分为三种：监督学习、无监督学习和强化学习。不同于监督学习，强化学习没有标签数据，只有一种叫reward（奖励）的标量信号评价动作的好坏，所以强化学习看起来很另类。
这一节先讲述基本概念。
### reward ###
reward译为“奖励”，是个体采取行动后环境的一个反馈。reward为强化学习设定一个目标：问题的解决可被描述为最大化累积奖励。

###agent
agent即“个体”，相当于机器人、游戏中的主角。在RL中个体可以对环境进行观察，决策出行动，随后获得一个从环境返回的奖励。

###environment
也就是环境。顾名思义，它就是相当于游戏中除主角之外的地图、怪物等集合。环境接收个体动作之后，对本身信息进行更新（也就是一个交互），并返回个体一个奖励。个体与环境的交互图如下：

###state
<p>译为“状态”，是历史信息的函数，包含所有已有的信息。根据客体的不同状态分为三种：环境状态、个体状态和信息状态。环境状态是环境所有的信息，对于个体来说有可观测和部分可观测之别。可观测就是个体知道环境的所有信息，相当于开启上帝视角。部分可观测指个体只了解环境的部分信息，相当于从人类的视角去探索。
<p>个体状态是个体可利用的所有信息，是个体历史信息的函数。
<p>信息状态比较概念化，目前对它一知半解。
<p>然后讲讲马尔科夫性，这是RL重要的一个思想基础，但是平时基本不会注意到它（因为太基础了）。<p>上面的条件概率等式的言外之意就是未来状态只取决于当前，与过去的所有信息无关！

<p>
<p>强化学习大部分情况下都是在训练个体，个体的组成也是重要的概念。一般来说个体由三部分组成：<p>
**policy**：译为“策略”，是状态到动作的函数。

<p>
**value function**：价值函数，是评价状态的一个指标。具体为未来奖励的期望值，即：

<p>
**model**：是个体对环境的建模，我的理解是相当于人类对环境的认知建模，主要包括状态转移概率

和奖励


##强化学习的分类
根据个体是否建立模型可分为model-based和model-free两大类，之后将会学习到还能进一步分类；根据使用的指标分为value-vased、policy-based和actor-critic。
